apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: churn-predictor
  namespace: mlops
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 3
    containers:
      - name: kserve-container
        image: ghcr.io/mlops-course/churn-predictor-api:latest
        ports:
          - containerPort: 8000
            protocol: TCP
        env:
          - name: MLFLOW_TRACKING_URI
            value: "http://mlflow:5000"
          - name: MLFLOW_S3_ENDPOINT_URL
            value: "http://minio:9000"
          - name: AWS_ACCESS_KEY_ID
            value: "minioadmin"
          - name: AWS_SECRET_ACCESS_KEY
            value: "minioadmin"
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 1
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
