{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering for Beginners - Interactive Tutorial\n",
    "\n",
    "**Welcome!** üëã\n",
    "\n",
    "This notebook is designed for **DevOps engineers** or anyone with **no data engineering background** who wants to understand what data scientists do.\n",
    "\n",
    "## What You'll Learn\n",
    "- üìä How to load and explore data\n",
    "- üßπ How to clean messy data\n",
    "- ‚öôÔ∏è How to create useful features\n",
    "- ü§ñ How to train your first ML model\n",
    "- üìà How to evaluate model performance\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of variables, functions, loops\n",
    "- No ML/DS experience required!\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "First, let's import the tools we'll need. Think of these as your toolkit for data work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd          # Like Excel for Python\n",
    "import numpy as np           # Math operations\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt    # Basic plotting\n",
    "import seaborn as sns              # Pretty plots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generate Sample Data\n",
    "\n",
    "Let's create some sample customer data to practice with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data generation script\n",
    "# This creates a CSV file with 10,000 customer records\n",
    "%run ../scripts/00_generate_sample_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load and Inspect Data\n",
    "\n",
    "Now let's load the data and take a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "# DataFrame = table with rows and columns\n",
    "df = pd.read_csv('../data/raw/customers.csv')\n",
    "\n",
    "print(f\"üìä Loaded {len(df)} customer records\")\n",
    "print(f\"üìã Dataset has {df.shape[1]} columns\")\n",
    "\n",
    "# Show first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"üìã Dataset Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for numeric columns\n",
    "print(\"üìà Summary Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Explore the Data\n",
    "\n",
    "**Task:** Answer these questions by exploring the data:\n",
    "1. How many customers are there?\n",
    "2. What is the average age?\n",
    "3. What percentage of customers churned?\n",
    "4. Which subscription type is most common?\n",
    "\n",
    "**Hints:**\n",
    "- Use `len(df)` for total rows\n",
    "- Use `df['column_name'].mean()` for average\n",
    "- Use `df['column_name'].value_counts()` for counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(f\"Total customers: {len(df)}\")\n",
    "print(f\"Average age: {df['age'].mean():.1f}\")\n",
    "print(f\"Churn rate: {df['churned'].mean():.1%}\")\n",
    "print(f\"\\nSubscription type counts:\")\n",
    "print(df['subscription_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's visualize the data to understand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "print(\"‚ùì Missing Values:\")\n",
    "for col, count, pct in zip(missing.index, missing.values, missing_pct.values):\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['age'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Distribution of Customers')\n",
    "plt.axvline(df['age'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"age\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count plot\n",
    "df['churned'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Churn Distribution')\n",
    "axes[0].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Not Churned', 'Churned'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['churned'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                   labels=['Not Churned', 'Churned'], colors=['green', 'red'], alpha=0.7)\n",
    "axes[1].set_title('Churn Proportion')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare churners vs non-churners\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Age comparison\n",
    "df.boxplot(column='age', by='churned', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age by Churn Status')\n",
    "axes[0, 0].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[0, 0].set_ylabel('Age')\n",
    "\n",
    "# Login count comparison\n",
    "df.boxplot(column='login_count', by='churned', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Login Count by Churn Status')\n",
    "axes[0, 1].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[0, 1].set_ylabel('Login Count')\n",
    "\n",
    "# Support tickets comparison\n",
    "df.boxplot(column='support_tickets', by='churned', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Support Tickets by Churn Status')\n",
    "axes[1, 0].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[1, 0].set_ylabel('Support Tickets')\n",
    "\n",
    "# Days as customer comparison\n",
    "df.boxplot(column='days_as_customer', by='churned', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Days as Customer by Churn Status')\n",
    "axes[1, 1].set_xlabel('Churned (0=No, 1=Yes)')\n",
    "axes[1, 1].set_ylabel('Days as Customer')\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"  - Do churners have fewer logins?\")\n",
    "print(\"  - Do churners have more support tickets?\")\n",
    "print(\"  - Are newer customers more likely to churn?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 2: Create Your Own Visualization\n",
    "\n",
    "**Task:** Create a visualization to compare `total_spent` between churners and non-churners.\n",
    "\n",
    "**Hint:** Use `df.boxplot(column='total_spent', by='churned')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Cleaning\n",
    "\n",
    "Real data is messy! Let's clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Starting with {len(df_clean)} rows\")\n",
    "\n",
    "# 1. Remove duplicates\n",
    "duplicates = df_clean.duplicated().sum()\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(f\"Removed {duplicates} duplicates\")\n",
    "\n",
    "# 2. Fix age outliers\n",
    "print(f\"\\nAge range before: {df_clean['age'].min()} - {df_clean['age'].max()}\")\n",
    "df_clean = df_clean[(df_clean['age'] >= 18) & (df_clean['age'] <= 100)]\n",
    "print(f\"Age range after: {df_clean['age'].min()} - {df_clean['age'].max()}\")\n",
    "\n",
    "# 3. Fix negative support tickets\n",
    "negative_tickets = (df_clean['support_tickets'] < 0).sum()\n",
    "df_clean = df_clean[df_clean['support_tickets'] >= 0]\n",
    "print(f\"\\nRemoved {negative_tickets} rows with negative support tickets\")\n",
    "\n",
    "# 4. Handle missing values\n",
    "print(f\"\\nMissing values before:\")\n",
    "print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])\n",
    "\n",
    "# Fill missing income with median\n",
    "df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
    "\n",
    "# For other missing values, we'll drop those rows\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "print(f\"\\nMissing values after: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"\\nFinal dataset: {len(df_clean)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feature Engineering\n",
    "\n",
    "Let's create new features that might help predict churn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to datetime\n",
    "df_clean['signup_date'] = pd.to_datetime(df_clean['signup_date'])\n",
    "df_clean['last_login'] = pd.to_datetime(df_clean['last_login'])\n",
    "\n",
    "# Create new features\n",
    "print(\"Creating new features...\")\n",
    "\n",
    "# 1. Days since last login\n",
    "df_clean['days_since_last_login'] = (pd.Timestamp.now() - df_clean['last_login']).dt.days\n",
    "\n",
    "# 2. Login frequency (logins per day)\n",
    "df_clean['login_frequency'] = df_clean['login_count'] / (df_clean['days_as_customer'] + 1)\n",
    "\n",
    "# 3. Spend per day\n",
    "df_clean['spend_per_day'] = df_clean['total_spent'] / (df_clean['days_as_customer'] + 1)\n",
    "\n",
    "# 4. Engagement score (0-1)\n",
    "df_clean['engagement_score'] = (\n",
    "    (df_clean['login_count'] / df_clean['login_count'].max()) * 0.5 +\n",
    "    (df_clean['avg_session_duration'] / df_clean['avg_session_duration'].max()) * 0.5\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created 4 new features\")\n",
    "\n",
    "# Show sample of new features\n",
    "df_clean[['login_frequency', 'spend_per_day', 'engagement_score', 'churned']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: Create Your Own Feature\n",
    "\n",
    "**Task:** Create a new feature called `support_per_login` that calculates support tickets per login.\n",
    "\n",
    "**Formula:** `support_tickets / (login_count + 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Prepare Data for Machine Learning\n",
    "\n",
    "Now let's prepare the data for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'age', 'income', 'days_as_customer', 'login_count',\n",
    "    'avg_session_duration', 'support_tickets', 'total_spent',\n",
    "    'monthly_charge', 'days_since_last_login', 'login_frequency',\n",
    "    'spend_per_day', 'engagement_score'\n",
    "]\n",
    "\n",
    "# Add one-hot encoded features\n",
    "df_encoded = pd.get_dummies(df_clean, columns=['gender', 'subscription_type'], prefix=['gender', 'sub'])\n",
    "\n",
    "# Get all feature columns\n",
    "all_features = feature_cols + [col for col in df_encoded.columns if col.startswith(('gender_', 'sub_'))]\n",
    "\n",
    "X = df_encoded[all_features]\n",
    "y = df_encoded['churned']\n",
    "\n",
    "print(f\"Features: {len(all_features)}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Churn rate: {y.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # For reproducibility\n",
    "    stratify=y            # Keep same churn ratio\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Train churn rate: {y_train.mean():.1%}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled (mean=0, std=1)\")\n",
    "print(f\"\\nExample - Before scaling: {X_train.iloc[0, 0]:.2f}\")\n",
    "print(f\"Example - After scaling: {X_train_scaled[0, 0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Train Your First Model! ü§ñ\n",
    "\n",
    "This is exciting - let's train a machine learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "print(\"üöÄ Training Random Forest model...\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,     # Number of trees\n",
    "    max_depth=10,         # Maximum tree depth\n",
    "    random_state=42,      # For reproducibility\n",
    "    n_jobs=-1             # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"‚úÖ Predictions made!\")\n",
    "print(f\"\\nExample predictions (first 10):\")\n",
    "print(f\"Actual:    {y_test.iloc[:10].values}\")\n",
    "print(f\"Predicted: {y_test_pred[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Evaluate the Model\n",
    "\n",
    "How well did our model do? Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"üìä Model Performance:\")\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.1%}\")\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nüìã Detailed Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Not Churned', 'Churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Churned', 'Churned'],\n",
    "            yticklabels=['Not Churned', 'Churned'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to read this:\")\n",
    "print(f\"  Top-left ({cm[0,0]}): Correctly predicted NOT churned\")\n",
    "print(f\"  Bottom-right ({cm[1,1]}): Correctly predicted churned\")\n",
    "print(f\"  Top-right ({cm[0,1]}): Incorrectly predicted churned (False Positive)\")\n",
    "print(f\"  Bottom-left ({cm[1,0]}): Incorrectly predicted NOT churned (False Negative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"üîù Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You just completed your first end-to-end data engineering pipeline!\n",
    "\n",
    "### What You Accomplished:\n",
    "‚úÖ Loaded and explored data\n",
    "‚úÖ Cleaned messy data\n",
    "‚úÖ Created useful features\n",
    "‚úÖ Prepared data for ML\n",
    "‚úÖ Trained a model\n",
    "‚úÖ Evaluated performance\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different features\n",
    "2. Experiment with model parameters\n",
    "3. Move on to Module 01: MLOps Foundations\n",
    "4. Learn about data versioning (DVC)\n",
    "5. Track experiments (MLflow)\n",
    "\n",
    "### üìö Resources:\n",
    "- [Module 00.5](../course/00.5-data-engineering-for-beginners.md) - Full guide\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Scikit-learn Tutorials](https://scikit-learn.org/stable/tutorial/)\n",
    "\n",
    "**Happy Learning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
